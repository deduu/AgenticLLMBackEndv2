large_models:
  # - model_type: "qwen"
  #   model_path: "Qwen/QwQ-32B-Preview"
  #   device: "auto"
  #   dtype: "float16"
  #   quantization: "4bit"  # Load with 4-bit quantization
  # - model_type: "llamma"
  #   model_path: "meta-llama/Llama-3.3-70B-Instruct"
  #   device: "auto"
  #   dtype: "float16"
  #   quantization: "4bit"  # Load with 4-bit quantization
medium_models:
  # - model_type: "llamma"
  #   model_path: "meta-llama/Llama-3.3-70B-Instruct"
  #   device: "auto"
  #   dtype: "float16"
  #   quantization: "4bit"  # Load with 4-bit quantization
  - model_type: "deepseek"
    model_path: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
    device: "cuda:1"
    dtype: "float16"
    quantization: "None"  # Load with 4-bit quantization

    
  # - model_type: "qwen"
  #   model_path: "Qwen/Qwen2.5-7B-Instruct"
  #   device: "cuda:0"
  #   dtype: "float16"
  # - model_type: "llamma"
  #   model_path: "meta-llama/Llama-3.1-8B"
  #   device: "cuda:1"
  #   dtype: "float16"
  #   parameter: "8B"
  #   quantization: "None"
  # - model_type: "llamma"
  #   model_path: "meta-llama/Llama-3.1-8B-Instruct"
  #   device: "cuda:1"
  #   dtype: "float16"
  #   parameter: "8B"
  #   quantization: "None"
  
small_models:
  # - model_type: "llamma_small"
  #   model_path: "meta-llama/Llama-3.2-3B-Instruct"
  #   device: "cuda:0"
  #   dtype: "float16"
  #   parameter: "None"
  - model_type: "llamma"
    model_path: "meta-llama/Llama-3.1-8B-Instruct"
    device: "cuda:4"
    dtype: "float16"
    parameter: "8B"
    quantization: "4bit"
    cot: "False"



embeddings:
  - model_id: "sentence-transformers/all-MiniLM-L6-v2"

