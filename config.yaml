models:
  - model_type: "qwen"
    model_path: "Qwen/QwQ-32B-Preview"
    device: "cuda:0"
    dtype: "float16"
    quantization: "4bit"  # Load with 4-bit quantization
    
  # - model_type: "qwen"
  #   model_path: "Qwen/Qwen2.5-7B-Instruct"
  #   device: "cuda:0"
  #   dtype: "float16"
  
  # - model_type: "llamma"
  #   model_path: "meta-llama/Llama-3.1-8B-Instruct"
  #   device: "cuda:0"
  #   dtype: "float16"
